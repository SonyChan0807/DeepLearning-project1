{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dlc_bci as bci\n",
    "from dlc_practical_prologue import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset (Dont convert to one hot labels if using cross entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_input, tr_target = bci.load(\"bci\", train=True, one_khz=False)\n",
    "te_input, te_target = bci.load(\"bci\", train=False, one_khz=False)\n",
    "tr_target_onehot = convert_to_one_hot_labels(tr_input, tr_target)\n",
    "te_target_onehot = convert_to_one_hot_labels(te_input, te_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_input = torch.nn.functional.normalize(tr_input, p=2, dim=0) \n",
    "te_input = torch.nn.functional.normalize(te_input, p=2, dim=0) \n",
    "\n",
    "tr_input, tr_target, tr_target_onehot = Variable(tr_input.view(-1,28*50)), Variable(tr_target), Variable(tr_target_onehot)\n",
    "te_input, te_target, te_target_onehot = Variable(te_input.view(-1,28*50)), Variable(te_target), Variable(te_target_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of NN is still a vector (cross entropy loss handles log-softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 50, 200)\n",
    "        self.fc2 = nn.Linear(200, 2)\n",
    "        \n",
    "    def forward(self, x, mode=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        #x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 tr loss 54.85 te loss 17.23\n",
      "epoch 1 tr loss 54.80 te loss 17.24\n",
      "epoch 2 tr loss 54.76 te loss 17.25\n",
      "epoch 3 tr loss 54.73 te loss 17.26\n",
      "epoch 4 tr loss 54.70 te loss 17.27\n",
      "epoch 5 tr loss 54.67 te loss 17.27\n",
      "epoch 6 tr loss 54.64 te loss 17.28\n",
      "epoch 7 tr loss 54.62 te loss 17.29\n",
      "epoch 8 tr loss 54.59 te loss 17.30\n",
      "epoch 9 tr loss 54.56 te loss 17.31\n",
      "epoch 10 tr loss 54.54 te loss 17.32\n",
      "epoch 11 tr loss 54.51 te loss 17.32\n",
      "epoch 12 tr loss 54.49 te loss 17.33\n",
      "epoch 13 tr loss 54.46 te loss 17.34\n",
      "epoch 14 tr loss 54.44 te loss 17.34\n",
      "epoch 15 tr loss 54.41 te loss 17.35\n",
      "epoch 16 tr loss 54.39 te loss 17.35\n",
      "epoch 17 tr loss 54.36 te loss 17.36\n",
      "epoch 18 tr loss 54.33 te loss 17.36\n",
      "epoch 19 tr loss 54.31 te loss 17.37\n",
      "epoch 20 tr loss 54.28 te loss 17.37\n",
      "epoch 21 tr loss 54.25 te loss 17.38\n",
      "epoch 22 tr loss 54.23 te loss 17.38\n",
      "epoch 23 tr loss 54.20 te loss 17.39\n",
      "epoch 24 tr loss 54.17 te loss 17.39\n",
      "tr error 38.29% 121/316\n",
      "te error 53.00% 53/100\n"
     ]
    }
   ],
   "source": [
    "# construct and train model\n",
    "model = Net()\n",
    "tr_loss, te_loss = bci.train_model(model, tr_input, tr_target, 4, te_input, te_target, 4, 25)\n",
    "torch.save(model.state_dict(), os.getcwd() + \"v1.pth\")\n",
    "\n",
    "# compute train and test errors\n",
    "nb_tr_errors = bci.compute_nb_errors(model, tr_input, tr_target_onehot, 4)\n",
    "nb_te_errors = bci.compute_nb_errors(model, te_input, te_target_onehot, 4)\n",
    "\n",
    "print('tr error {:0.2f}% {:d}/{:d}'.format((100 * nb_tr_errors) / tr_input.size(0), nb_tr_errors, tr_input.size(0)))\n",
    "print('te error {:0.2f}% {:d}/{:d}'.format((100 * nb_te_errors) / te_input.size(0), nb_te_errors, te_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 150\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_loss, label='training loss')\n",
    "plt.plot(te_loss, label='validation loss')\n",
    "plt.legend(loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
